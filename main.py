# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DUaXdbJFrHR8KVt5tpZhGLIOsrXF1jEv
"""

import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import time
from torch.optim import Adam
from torch.nn import CrossEntropyLoss
import os
import argparse
import torch
import torch.nn as nn
import dataset
from dataset import Shakespeare
import model
from model import CharRNN, CharLSTM
from matplotlib import pyplot as plt

class ShakespeareDataset(torch.utils.data.Dataset):
    def __init__(self, input_file, sequence_length=30):
        with open(input_file, 'r') as f:
            self.data = f.read()

        self.chars = sorted(list(set(self.data)))
        self.char_to_index = {char: idx for idx, char in enumerate(self.chars)}
        self.index_to_char = {idx: char for idx, char in enumerate(self.chars)}
        self.input_size = len(self.chars)
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) // self.sequence_length

    def __getitem__(self, idx):
        start_idx = idx * self.sequence_length
        end_idx = start_idx + self.sequence_length + 1
        text_str = self.data[start_idx:end_idx]
        input_seq = text_str[:-1]
        target_seq = text_str[1:]

        input_seq = torch.tensor([self.char_to_index[c] for c in input_seq], dtype=torch.long)
        target_seq = torch.tensor([self.char_to_index[c] for c in target_seq], dtype=torch.long)

        return input_seq, target_seq

class CharRNN(nn.Module):
    def __init__(self, input_size, hidden_dim, n_layers=1):
        super(CharRNN, self).__init__()
        self.input_size = input_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim

        self.embed = nn.Embedding(input_size, hidden_dim)
        self.RNN = nn.RNN(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, self.input_size)

    def forward(self, input, hidden):
        embedded = self.embed(input)
        output, hidden = self.RNN(embedded, hidden)
        output = output.contiguous().view(-1, self.hidden_dim)
        output = self.fc(output)
        return output, hidden

    def init_hidden(self, batch_size):
        initial_hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)
        return initial_hidden

class CharLSTM(nn.Module):
    def __init__(self, input_size, hidden_dim, n_layers=1):
        super(CharLSTM, self).__init__()
        self.input_size = input_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.embed = nn.Embedding(input_size, hidden_dim)
        self.LSTM = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, input_size)

    def forward(self, input, hidden):
        embedded = self.embed(input)
        output, hidden = self.LSTM(embedded, hidden)
        output = output.contiguous().view(-1, self.hidden_dim)
        output = self.fc(output)
        return output, hidden

    def init_hidden(self, batch_size):
        initial_hidden = (
            torch.zeros(self.n_layers, batch_size, self.hidden_dim),
            torch.zeros(self.n_layers, batch_size, self.hidden_dim)
        )
        return initial_hidden

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
datasets = ShakespeareDataset(input_file='shakespeare_train.txt')
chars = datasets.chars

def train(model, trn_loader, device, criterion, optimizer, model_name):
    model.to(device)
    model.train()

    trn_loss = 0

    for input, target in trn_loader:
        input, target = input.to(device), target.to(device)

        batch_size = input.size(0)

        if model_name == 'RNN':
            hidden = model.init_hidden(batch_size=batch_size).to(device)
        elif model_name == 'LSTM':
            hidden = model.init_hidden(batch_size=batch_size)
            hidden = (hidden[0].to(device), hidden[1].to(device))

        optimizer.zero_grad()
        output, hidden = model(input, hidden)

        loss = criterion(output.view(-1, model.input_size), target.view(-1))
        loss.backward()
        optimizer.step()

        trn_loss += loss.item()

    trn_loss = trn_loss / len(trn_loader)

    return trn_loss

def validate(model, val_loader, device, criterion, model_name):
    model.to(device)
    model.eval()

    val_loss = 0

    with torch.no_grad():
        for input, target in val_loader:
            input, target = input.to(device), target.to(device)

            batch_size = input.size(0)

            if model_name == 'RNN':
                hidden = model.init_hidden(batch_size=batch_size).to(device)
            elif model_name == 'LSTM':
                hidden = model.init_hidden(batch_size=batch_size)
                hidden = (hidden[0].to(device), hidden[1].to(device))

            output, hidden = model(input, hidden)
            loss = criterion(output.view(-1, model.input_size), target.view(-1))

            val_loss += loss.item()

    val_loss = val_loss / len(val_loader)

    return val_loss

def main(epochs, model_name, batch_size, hidden_dim):
    data = datasets

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    plot_save_dir = './img'
    model_save_dir = './model'

    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)

    input_size = len(data.chars)

    index_list = list(range(len(data)))
    np.random.shuffle(index_list)
    split = int(np.floor(0.8 * len(data)))

    train_sampler = SubsetRandomSampler(indices=index_list[:split])
    valid_sampler = SubsetRandomSampler(indices=index_list[split:])

    train_dataloader = DataLoader(dataset=data, batch_size=batch_size, sampler=train_sampler)
    valid_dataloader = DataLoader(dataset=data, batch_size=batch_size, sampler=valid_sampler)

    train_losses = []
    val_losses = []

    if model_name == 'RNN':
        print(f'Training RNN using {device}...')

        model = CharRNN(input_size=input_size, hidden_dim=hidden_dim)
        criterion  = CrossEntropyLoss()
        optimizer = Adam(params=model.parameters())

        for epoch in range(epochs):
            print(f'Epoch: [{epoch+1}/{epochs}]')

            train_loss = train(model=model, trn_loader=train_dataloader, device=device, criterion=criterion, optimizer=optimizer, model_name=model_name)
            train_losses.append(train_loss)

            val_loss = validate(model=model, val_loader=valid_dataloader, device=device, criterion=criterion, model_name=model_name)
            val_losses.append(val_loss)

            print(f'Train Loss: {train_loss}', '\t', f'Valid Loss: {val_loss}')

    if model_name == 'LSTM':
        print(f'Training LSTM using {device}...')

        model = CharLSTM(input_size=input_size, hidden_dim=hidden_dim)
        criterion = CrossEntropyLoss()
        optimizer = Adam(params=model.parameters())

        for epoch in range(epochs):
            print(f'Epoch: [{epoch+1}/{epochs}]')

            train_loss = train(model=model, trn_loader=train_dataloader, device=device, criterion=criterion, optimizer=optimizer, model_name=model_name)
            train_losses.append(train_loss)

            val_loss = validate(model=model, val_loader=valid_dataloader, device=device, criterion=criterion, model_name=model_name)
            val_losses.append(val_loss)

            print(f'Train Loss: {train_loss}', '\t', f'Valid Loss: {val_loss}')

    plt.figure()
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')

    if model_name == 'RNN':
        plt.title('RNN Training and Validation Loss')
    elif model_name == 'LSTM':
        plt.title('LSTM Training and Validation Loss')

    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    if not os.path.exists(plot_save_dir):
        os.makedirs(plot_save_dir)
    plt.savefig(os.path.join(plot_save_dir, f'{model_name}.png'))

    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    torch.save(model.state_dict(), os.path.join(model_save_dir, f'{model_name}.pth'))

    return device, model, data

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--model_name', type=str, required=True, help='RNN or LSTM')
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--hidden_dim', type=int, default=128)
    args = parser.parse_args()

    epochs = args.epochs
    model_name = args.model_name
    batch_size = args.batch_size